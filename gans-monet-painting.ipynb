{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":21755,"databundleVersionId":1475600,"sourceType":"competition"}],"dockerImageVersionId":30684,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Paint Like Monet? AI Art Challenge with GANs\n\nIn this contest, our aim is to harness the power of Generative Adversarial Networks (GANs) to replicate Claude Monet's distinctive artistic style in digital imagery. GANs are comprised of a generator and a discriminator: the generator crafts images in the style of Monet, while the discriminator discerns between authentic and synthesized images. Our objective is to produce between 7,000 to 10,000 Monet-esque images, which will be assessed using the **MiFID (Memorization-informed Fr√©chet Inception Distance)** metric.\n\nGANs, in particular, represent a robust category of generative models featuring two neural networks: a generator and a discriminator. Through adversarial training, the generator aims to create realistic data samples, while the discriminator strives to differentiate between real and fake ones. This ongoing back-and-forth fosters the generator's ability to generate high-quality, lifelike data.","metadata":{}},{"cell_type":"markdown","source":"## DATA\n\nDataset is organized into four directories: monet_tfrec, photo_tfrec, monet_jpg, and photo_jpg. Both monet_tfrec and monet_jpg hold identical painting images, while photo_tfrec and photo_jpg contain matching sets of photos.\n\nWe suggest utilizing TFRecords, as this competition offers an opportunity to familiarize oneself with this data format. However, JPEG images are also provided for convenience.\n\nFocus your model training on the images within the monet directories, which comprise Monet paintings. These will serve as the basis for teaching your model.\n\nThe photo directories contain various photos that need to be imbued with Monet-style characteristics. Your task is to infuse these images with Monet's artistic style and submit the resulting JPEGs in a zip file. Ensure that your submission contains no more than 10,000 images.\n\nIt's worth noting that Monet-style art can be crafted from scratch using alternative GAN architectures such as DCGAN. Therefore, the submitted image files do not necessarily have to be transformed photos from the provided dataset.\n\nhttps://www.kaggle.com/competitions/gan-getting-started/data","metadata":{}},{"cell_type":"code","source":"import os\nimport math\nimport time\nimport shutil\n\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport cv2\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nimport torch.nn.init as init\nfrom torch.utils.data import Dataset, random_split, DataLoader\n\nimport torchvision.models as models\nimport torchvision.transforms as transforms\n\nfrom tqdm.notebook import tqdm\nimport itertools","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:07.991075Z","iopub.execute_input":"2024-04-12T20:20:07.991472Z","iopub.status.idle":"2024-04-12T20:20:15.916379Z","shell.execute_reply.started":"2024-04-12T20:20:07.991441Z","shell.execute_reply":"2024-04-12T20:20:15.915473Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"BASE_INPUT_PATH = '/kaggle/input/gan-getting-started/'\n\n\nMONET_PATH = os.path.join(BASE_INPUT_PATH, \"monet_jpg\")\nPHOTO_PATH = os.path.join(BASE_INPUT_PATH, \"photo_jpg\")\nOUTPUT_PATH = os.path.join('/kaggle/images')\n\nprint(f\"Monet paintings count : {len(os.listdir(MONET_PATH))}\")","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:28:24.125614Z","iopub.execute_input":"2024-04-12T20:28:24.126429Z","iopub.status.idle":"2024-04-12T20:28:24.138076Z","shell.execute_reply.started":"2024-04-12T20:28:24.126394Z","shell.execute_reply":"2024-04-12T20:28:24.136839Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Exploratory Data Analysis (EDA)\n\n* Monets, Photos sized 256x256 in JPEG format","metadata":{}},{"cell_type":"code","source":"def display_images_grid(directory_path, num_samples=9):\n    \"\"\"\n    Helper method display images in GRID\n    \"\"\"\n    image_files = os.listdir(directory_path)[:num_samples]\n    \n    num_cols = int(math.sqrt(num_samples))\n    num_rows = math.ceil(num_samples / num_cols)\n    \n    fig, axes = plt.subplots(num_rows, num_cols, figsize=(12, 12))\n    \n    for i, image_name in enumerate(image_files):\n        img = cv2.imread(os.path.join(directory_path, image_name))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        if num_rows == 1:\n            ax = axes[i % num_cols]\n        else:\n            ax = axes[i // num_cols, i % num_cols]\n        \n        ax.imshow(img)\n        ax.axis('off')\n    \n    plt.tight_layout()\n    plt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:16.236748Z","iopub.execute_input":"2024-04-12T20:20:16.237139Z","iopub.status.idle":"2024-04-12T20:20:16.245837Z","shell.execute_reply.started":"2024-04-12T20:20:16.237109Z","shell.execute_reply":"2024-04-12T20:20:16.244725Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sample Monet Images","metadata":{}},{"cell_type":"code","source":"display_images_grid(MONET_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:16.248686Z","iopub.execute_input":"2024-04-12T20:20:16.249087Z","iopub.status.idle":"2024-04-12T20:20:18.346125Z","shell.execute_reply.started":"2024-04-12T20:20:16.249049Z","shell.execute_reply":"2024-04-12T20:20:18.342455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Sample Photos","metadata":{}},{"cell_type":"code","source":"display_images_grid(PHOTO_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:18.348199Z","iopub.execute_input":"2024-04-12T20:20:18.348690Z","iopub.status.idle":"2024-04-12T20:20:20.073381Z","shell.execute_reply.started":"2024-04-12T20:20:18.348643Z","shell.execute_reply":"2024-04-12T20:20:20.071455Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Model Building\n\nAs we are familiar with, a Generative Adversarial Network (GAN) is a machine learning model comprising a generator and a discriminator engaged in a competitive process to generate realistic data. In this project, we will employ CycleGAN, a specific variant of GAN designed for image-to-image translation tasks lacking paired data. CycleGAN utilizes two generators and two discriminators to facilitate the translation of images from one domain to another while maintaining consistency in reversibility. \n\nThis approach finds widespread application in tasks such as style transfer and image transformation.","metadata":{}},{"cell_type":"markdown","source":"### Dataset\n\nWe establish a dataset for training our generative model. It is designed to accommodate the two directories containing our Monet paintings and photos. This class is responsible for loading and preprocessing the images, which includes resizing and optionally normalization, to ensure uniform input for our neural network. Through the __getitem__ method, we randomly select a Monet-style image and its corresponding photo, process them, and return them as tensors. Additionally, the __len__ method ensures that the dataset size is constrained by the number of available pairs, returning the length of the smaller set of images.","metadata":{}},{"cell_type":"code","source":"class ImageDataset(Dataset):\n    def __init__(self, path_monet, path_photo, size=(256, 256), normalize=True):\n        super().__init__()\n        self.monet_dir = path_monet\n        self.photo_dir = path_photo\n        self.monet_files = os.listdir(self.monet_dir)\n        self.photo_files = os.listdir(self.photo_dir)\n        \n        if normalize:\n            self.transform = transforms.Compose([\n                transforms.Resize(size),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                \n            ])\n        else:\n            self.transform = transforms.Compose([\n                transforms.Resize(size),\n                transforms.ToTensor()])\n        \n        self.monet_idx = {i: fl for i, fl in enumerate(os.listdir(self.monet_dir))}\n        self.photo_idx = {i: fl for i, fl in enumerate(os.listdir(self.photo_dir))}\n            \n    def __getitem__(self, idx):\n        \"\"\"\n        Randomly select pair of photo and monet\n        \"\"\"\n        rand_idx = np.random.randint(0, len(self.monet_files))\n        photo_path = os.path.join(self.photo_dir, self.photo_files[idx % len(self.photo_files)])\n        monet_path = os.path.join(self.monet_dir, self.monet_files[rand_idx])\n        \n        photo_img = Image.open(photo_path)\n        photo_img = self.transform(photo_img)\n        \n        monet_img = Image.open(monet_path)\n        monet_img = self.transform(monet_img)\n        \n        return photo_img, monet_img\n\n    def __len__(self):\n        return min(len(self.monet_files), len(self.photo_files))\n","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.075469Z","iopub.execute_input":"2024-04-12T20:20:20.076413Z","iopub.status.idle":"2024-04-12T20:20:20.092916Z","shell.execute_reply.started":"2024-04-12T20:20:20.076373Z","shell.execute_reply":"2024-04-12T20:20:20.091535Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_ds = ImageDataset(MONET_PATH, PHOTO_PATH)\nprint(f\"Length: {image_ds.__len__()}\")\n\nimage_dl = DataLoader(image_ds, batch_size=1, pin_memory=True)\n\n# Using GPU\ndevice = 'cuda' if torch.cuda.is_available() else 'cpu'","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.094138Z","iopub.execute_input":"2024-04-12T20:20:20.094505Z","iopub.status.idle":"2024-04-12T20:20:20.155509Z","shell.execute_reply.started":"2024-04-12T20:20:20.094476Z","shell.execute_reply":"2024-04-12T20:20:20.154307Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Cycle GAN\n\n**CycleGAN** is a type of neural network used for image-to-image translation tasks. It can transform images from one domain to another without needing paired examples for training. Instead of pairs, it uses cycle-consistency, where images translated back and forth between domains should resemble the originals. This approach makes it useful for various tasks like style transfer or changing seasons in images.","metadata":{}},{"cell_type":"markdown","source":"Reverse normalization operation to restore the original image from its normalized form","metadata":{}},{"cell_type":"code","source":"def reverse_normalize(img, mean=[0.5, 0.5, 0.5], std=[0.5, 0.5, 0.5]):\n    for channel, mu, sigma in zip(img, mean, std):\n        channel.mul_(sigma).add_(mu)\n        \n    return img","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.157152Z","iopub.execute_input":"2024-04-12T20:20:20.157511Z","iopub.status.idle":"2024-04-12T20:20:20.163393Z","shell.execute_reply.started":"2024-04-12T20:20:20.157481Z","shell.execute_reply":"2024-04-12T20:20:20.162357Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Upsample(in_ch, out_ch, use_dropout=True, dropout_ratio=0.5):\n    if use_dropout:\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_ch, out_ch, 3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(out_ch),\n            nn.Dropout(dropout_ratio),\n            nn.GELU())\n    else:\n        return nn.Sequential(\n            nn.ConvTranspose2d(in_ch, out_ch, 3, stride=2, padding=1, output_padding=1),\n            nn.InstanceNorm2d(out_ch),\n            nn.GELU())","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.164758Z","iopub.execute_input":"2024-04-12T20:20:20.165071Z","iopub.status.idle":"2024-04-12T20:20:20.175855Z","shell.execute_reply.started":"2024-04-12T20:20:20.165044Z","shell.execute_reply":"2024-04-12T20:20:20.174708Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def Convlayer(in_ch, out_ch, kernel_size=3, stride=2, use_leaky=True, use_inst_norm=True, use_pad=True):\n    if use_pad:\n        conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, 1, bias=True)\n    else:\n        conv = nn.Conv2d(in_ch, out_ch, kernel_size, stride, 0, bias=True)\n\n    if use_leaky:\n        actv = nn.LeakyReLU(negative_slope=0.2, inplace=True)\n    else:\n        actv = nn.GELU()\n\n    if use_inst_norm:\n        norm = nn.InstanceNorm2d(out_ch)\n    else:\n        norm = nn.BatchNorm2d(out_ch)\n\n    return nn.Sequential(conv, norm, actv)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.179690Z","iopub.execute_input":"2024-04-12T20:20:20.180010Z","iopub.status.idle":"2024-04-12T20:20:20.191343Z","shell.execute_reply.started":"2024-04-12T20:20:20.179984Z","shell.execute_reply":"2024-04-12T20:20:20.190145Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Resblock(nn.Module):\n    def __init__(self, in_features, use_dropout=True, dropout_ratio=0.5):\n        super().__init__()\n        layers = list()\n        layers.append(nn.ReflectionPad2d(1))\n        layers.append(Convlayer(in_features, in_features, 3, 1, False, use_pad=False))\n        layers.append(nn.Dropout(dropout_ratio))\n        layers.append(nn.ReflectionPad2d(1))\n        layers.append(nn.Conv2d(in_features, in_features, 3, 1, padding=0, bias=True))\n        layers.append(nn.InstanceNorm2d(in_features))\n        self.res = nn.Sequential(*layers)\n\n    def forward(self, x):\n        return x + self.res(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.192723Z","iopub.execute_input":"2024-04-12T20:20:20.193050Z","iopub.status.idle":"2024-04-12T20:20:20.203438Z","shell.execute_reply.started":"2024-04-12T20:20:20.193024Z","shell.execute_reply":"2024-04-12T20:20:20.202474Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Generator(nn.Module):\n    def __init__(self, in_ch, out_ch, num_res_blocks=6):\n        super().__init__()\n        model = list()\n        model.append(nn.ReflectionPad2d(3))\n        model.append(Convlayer(in_ch, 64, 7, 1, False, True, False))\n        model.append(Convlayer(64, 128, 3, 2, False))\n        model.append(Convlayer(128, 256, 3, 2, False))\n        for _ in range(num_res_blocks):\n            model.append(Resblock(256))\n        model.append(Upsample(256, 128))\n        model.append(Upsample(128, 64))\n        model.append(nn.ReflectionPad2d(3))\n        model.append(nn.Conv2d(64, out_ch, kernel_size=7, padding=0))\n        model.append(nn.Tanh())\n\n        self.gen = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.gen(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.204771Z","iopub.execute_input":"2024-04-12T20:20:20.205193Z","iopub.status.idle":"2024-04-12T20:20:20.215889Z","shell.execute_reply.started":"2024-04-12T20:20:20.205153Z","shell.execute_reply":"2024-04-12T20:20:20.214878Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Discriminator(nn.Module):\n    def __init__(self, in_ch, num_layers=4):\n        super().__init__()\n        model = list()\n        model.append(nn.Conv2d(in_ch, 64, 4, stride=2, padding=1))\n        model.append(nn.LeakyReLU(0.2, inplace=True))\n        for i in range(1, num_layers):\n            in_chs = 64 * 2**(i-1)\n            out_chs = in_chs * 2\n            if i == num_layers -1:\n                model.append(Convlayer(in_chs, out_chs, 4, 1))\n            else:\n                model.append(Convlayer(in_chs, out_chs, 4, 2))\n        model.append(nn.Conv2d(512, 1, kernel_size=4, stride=1, padding=1))\n        self.disc = nn.Sequential(*model)\n\n    def forward(self, x):\n        return self.disc(x)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.217107Z","iopub.execute_input":"2024-04-12T20:20:20.217477Z","iopub.status.idle":"2024-04-12T20:20:20.231002Z","shell.execute_reply.started":"2024-04-12T20:20:20.217448Z","shell.execute_reply":"2024-04-12T20:20:20.230036Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def init_weights(net, init_type='normal', std=0.02):\n    def init_func(m):\n        classname = m.__class__.__name__\n        if hasattr(m, 'weight') and (classname.find('Conv') != -1 or classname.find('Linear') != -1):\n            init.normal_(m.weight.data, 0.0, std)\n            if hasattr(m, 'bias') and m.bias is not None:\n                init.constant_(m.bias.data, 0.0)\n        elif classname.find('BatchNorm2d') != -1:\n            init.normal_(m.weight.data, 1.0, std)\n            init.constant_(m.bias.data, 0.0)\n    net.apply(init_func)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.232319Z","iopub.execute_input":"2024-04-12T20:20:20.232694Z","iopub.status.idle":"2024-04-12T20:20:20.242810Z","shell.execute_reply.started":"2024-04-12T20:20:20.232658Z","shell.execute_reply":"2024-04-12T20:20:20.241745Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The \"sample fake\" mechanism in CycleGAN serves to enhance training stability by selecting a subset of fabricated images and utilizing them as inputs for the discriminator. This method mitigates input variability, curbing substantial fluctuations in discriminator outputs across training iterations. By managing the number of fake images per iteration and introducing randomness in their selection, this mechanism fosters stability and facilitates smoother convergence in CycleGAN training.\n\nClass designed to store 50 fabricated images and efficiently sample from them for input into the discriminator.","metadata":{}},{"cell_type":"code","source":"class sample_fake(object):\n    def __init__(self, max_imgs=50):\n        self.max_imgs = max_imgs\n        self.cur_img = 0\n        self.imgs = list()\n\n    def __call__(self, imgs):\n        ret = list()\n        for img in imgs:\n            if self.cur_img < self.max_imgs:\n                self.imgs.append(img)\n                ret.append(img)\n                self.cur_img += 1\n            else:\n                if np.random.ranf() > 0.5:\n                    idx = np.random.randint(0, self.max_imgs)\n                    ret.append(self.imgs[idx])\n                    self.imgs[idx] = img\n                else:\n                    ret.append(img)\n        return ret","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.244438Z","iopub.execute_input":"2024-04-12T20:20:20.244799Z","iopub.status.idle":"2024-04-12T20:20:20.254608Z","shell.execute_reply.started":"2024-04-12T20:20:20.244770Z","shell.execute_reply":"2024-04-12T20:20:20.253661Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"\ndef update_req_grad(models, requires_grad=True):\n    \"\"\"\n    Model parameters should be trainable or not\n    \"\"\"\n    for model in models:\n        for param in model.parameters():\n            param.requires_grad = requires_grad","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.255824Z","iopub.execute_input":"2024-04-12T20:20:20.256159Z","iopub.status.idle":"2024-04-12T20:20:20.268893Z","shell.execute_reply.started":"2024-04-12T20:20:20.256124Z","shell.execute_reply":"2024-04-12T20:20:20.267987Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Learning rate scheduling dynamically adjusts the learning rate during training to optimize neural network convergence. Typically, it involves gradually reducing the learning rate over time, aiding smoother convergence and reaching a better minimum for the loss function. Employing a linear decay strategy, the learning rate diminishes gradually as training advances beyond a certain threshold, enhancing training stability effectively.","metadata":{}},{"cell_type":"code","source":"class learning_rate_scheduling():\n    def __init__(self, decay_epochs=100, total_epochs=200):\n        self.decay_epochs = decay_epochs\n        self.total_epochs = total_epochs\n\n    def step(self, epoch_num):\n        if epoch_num <= self.decay_epochs:\n            return 1.0\n        else:\n            fract = (epoch_num - self.decay_epochs)  / (self.total_epochs - self.decay_epochs)\n            return 1.0 - fract","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.269948Z","iopub.execute_input":"2024-04-12T20:20:20.270215Z","iopub.status.idle":"2024-04-12T20:20:20.280453Z","shell.execute_reply.started":"2024-04-12T20:20:20.270192Z","shell.execute_reply":"2024-04-12T20:20:20.279464Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"Class (**AvgStats**) is crafted to accumulate and monitor key metrics, such as loss values and iteration counts, throughout the training process. These metrics serve diverse purposes, such as tracking training progress, visualizing loss trends, and informing decisions regarding model training strategies.","metadata":{}},{"cell_type":"code","source":"class AvgStats(object):\n    \"\"\"\n    Class to save training metrics\n    \"\"\"\n    def __init__(self):\n        self.reset()\n        \n    def reset(self):\n        self.losses = []\n        self.iterations = []\n        \n    def append(self, loss, iteration):\n        self.losses.append(loss)\n        self.iterations.append(iteration)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.281689Z","iopub.execute_input":"2024-04-12T20:20:20.282085Z","iopub.status.idle":"2024-04-12T20:20:20.291393Z","shell.execute_reply.started":"2024-04-12T20:20:20.282047Z","shell.execute_reply":"2024-04-12T20:20:20.290442Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The subsequent class encapsulates the **Cycle GAN framework** along with its associated training loop. Its objective is to train the model in translating images between two domains, namely photos and Monet paintings. The class encompasses the training logic for both generators and discriminators, incorporates learning rate scheduling, and facilitates tracking of training statistics. Throughout the training process, the model's losses are backpropagated, and parameters are updated accordingly to optimize performance.","metadata":{}},{"cell_type":"code","source":"class CycleGAN(object):\n    def __init__(self, in_ch, out_ch, epochs, start_lr=2e-4, lmbda=10, idt_coef=0.5, decay_epoch=0):\n        self.epochs = epochs\n        self.decay_epoch = decay_epoch if decay_epoch > 0 else int(self.epochs/2)\n        self.lmbda = lmbda\n        self.idt_coef = idt_coef\n        self.device = torch.device(device)\n        self.gen_mtp = Generator(in_ch, out_ch)\n        self.gen_ptm = Generator(in_ch, out_ch)\n        self.desc_m = Discriminator(in_ch)\n        self.desc_p = Discriminator(in_ch)\n        self.init_models()\n        self.mse_loss = nn.MSELoss()\n        self.l1_loss = nn.L1Loss()\n        self.adam_gen = torch.optim.Adam(itertools.chain(self.gen_mtp.parameters(), self.gen_ptm.parameters()),\n                                         lr = start_lr, betas=(0.5, 0.999))\n        self.adam_desc = torch.optim.Adam(itertools.chain(self.desc_m.parameters(), self.desc_p.parameters()),\n                                          lr=start_lr, betas=(0.5, 0.999))\n        self.sample_monet = sample_fake()\n        self.sample_photo = sample_fake()\n        gen_lr = learning_rate_scheduling(self.decay_epoch, self.epochs)\n        desc_lr = learning_rate_scheduling(self.decay_epoch, self.epochs)\n        self.gen_lr_sched = torch.optim.lr_scheduler.LambdaLR(self.adam_gen, gen_lr.step)\n        self.desc_lr_sched = torch.optim.lr_scheduler.LambdaLR(self.adam_desc, desc_lr.step)\n        self.gen_stats = AvgStats()\n        self.desc_stats = AvgStats()\n        \n    def init_models(self):\n        init_weights(self.gen_mtp)\n        init_weights(self.gen_ptm)\n        init_weights(self.desc_m)\n        init_weights(self.desc_p)\n        self.gen_mtp = self.gen_mtp.to(self.device)\n        self.gen_ptm = self.gen_ptm.to(self.device)\n        self.desc_m = self.desc_m.to(self.device)\n        self.desc_p = self.desc_p.to(self.device)\n        \n    def train(self, photo_dl):\n        for epoch in range(self.epochs):\n            start_time = time.time()\n            avg_gen_loss = 0.0\n            avg_desc_loss = 0.0\n            t = tqdm(photo_dl, leave=False, total=photo_dl.__len__())\n            for i, (photo_real, monet_real) in enumerate(t):\n                photo_img, monet_img = photo_real.to(self.device), monet_real.to(self.device)\n                update_req_grad([self.desc_m, self.desc_p], False)\n                self.adam_gen.zero_grad()\n\n                # forward pass through generator\n                fake_photo = self.gen_mtp(monet_img)\n                fake_monet = self.gen_ptm(photo_img)\n\n                cycl_monet = self.gen_ptm(fake_photo)\n                cycl_photo = self.gen_mtp(fake_monet)\n\n                id_monet = self.gen_ptm(monet_img)\n                id_photo = self.gen_mtp(photo_img)\n\n                # generator losses\n                idt_loss_monet = self.l1_loss(id_monet, monet_img) * self.lmbda * self.idt_coef\n                idt_loss_photo = self.l1_loss(id_photo, photo_img) * self.lmbda * self.idt_coef\n\n                cycle_loss_monet = self.l1_loss(cycl_monet, monet_img) * self.lmbda\n                cycle_loss_photo = self.l1_loss(cycl_photo, photo_img) * self.lmbda\n\n                monet_desc = self.desc_m(fake_monet)\n                photo_desc = self.desc_p(fake_photo)\n\n                real = torch.ones(monet_desc.size()).to(self.device)\n\n                adv_loss_monet = self.mse_loss(monet_desc, real)\n                adv_loss_photo = self.mse_loss(photo_desc, real)\n\n                # total generator loss\n                total_gen_loss = cycle_loss_monet + adv_loss_monet\\\n                              + cycle_loss_photo + adv_loss_photo\\\n                              + idt_loss_monet + idt_loss_photo\n                \n                avg_gen_loss += total_gen_loss.item()\n\n                # backward pass\n                total_gen_loss.backward()\n                self.adam_gen.step()\n\n                # forward pass through Descriminator\n                update_req_grad([self.desc_m, self.desc_p], True)\n                self.adam_desc.zero_grad()\n\n                fake_monet = self.sample_monet([fake_monet.cpu().data.numpy()])[0]\n                fake_photo = self.sample_photo([fake_photo.cpu().data.numpy()])[0]\n                fake_monet = torch.tensor(fake_monet).to(self.device)\n                fake_photo = torch.tensor(fake_photo).to(self.device)\n\n                monet_desc_real = self.desc_m(monet_img)\n                monet_desc_fake = self.desc_m(fake_monet)\n                photo_desc_real = self.desc_p(photo_img)\n                photo_desc_fake = self.desc_p(fake_photo)\n\n                real = torch.ones(monet_desc_real.size()).to(self.device)\n                fake = torch.zeros(monet_desc_fake.size()).to(self.device)\n\n                # descriminator losses\n                monet_desc_real_loss = self.mse_loss(monet_desc_real, real)\n                monet_desc_fake_loss = self.mse_loss(monet_desc_fake, fake)\n                photo_desc_real_loss = self.mse_loss(photo_desc_real, real)\n                photo_desc_fake_loss = self.mse_loss(photo_desc_fake, fake)\n\n                monet_desc_loss = (monet_desc_real_loss + monet_desc_fake_loss) / 2\n                photo_desc_loss = (photo_desc_real_loss + photo_desc_fake_loss) / 2\n                total_desc_loss = monet_desc_loss + photo_desc_loss\n                avg_desc_loss += total_desc_loss.item()\n\n                # backward\n                monet_desc_loss.backward()\n                photo_desc_loss.backward()\n                self.adam_desc.step()\n                \n                t.set_postfix(gen_loss=total_gen_loss.item(), desc_loss=total_desc_loss.item())\n            \n            avg_gen_loss /= photo_dl.__len__()\n            avg_desc_loss /= photo_dl.__len__()\n            time_req = time.time() - start_time\n            \n            self.gen_stats.append(avg_gen_loss, time_req)\n            self.desc_stats.append(avg_desc_loss, time_req)\n            \n            print(f\"Epoch {epoch+1}  -  Generator Loss: {avg_gen_loss}  -  Discriminator Loss: {avg_desc_loss}\")\n      \n            self.gen_lr_sched.step()\n            self.desc_lr_sched.step()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.294278Z","iopub.execute_input":"2024-04-12T20:20:20.294623Z","iopub.status.idle":"2024-04-12T20:20:20.327529Z","shell.execute_reply.started":"2024-04-12T20:20:20.294584Z","shell.execute_reply":"2024-04-12T20:20:20.326300Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"### Training\n\nWith all necessary building blocks now defined, we can proceed to train our model. We instantiate a new object of our Cycle GAN class and initiate the training process with 50 epochs.","metadata":{}},{"cell_type":"code","source":"# epochs = 50 # for final submission\nepochs = 5 # to make it faster\n\ngan = CycleGAN(3, 3, epochs)\ngan.train(image_dl)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:20:20.328710Z","iopub.execute_input":"2024-04-12T20:20:20.329030Z","iopub.status.idle":"2024-04-12T20:27:19.434483Z","shell.execute_reply.started":"2024-04-12T20:20:20.329003Z","shell.execute_reply":"2024-04-12T20:27:19.433528Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.xlabel(\"Epochs\")\nplt.ylabel(\"Losses\")\nplt.plot(gan.gen_stats.losses, 'r', label='Generator Loss')\nplt.plot(gan.desc_stats.losses, 'b', label='Descriminator Loss')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:27:39.813376Z","iopub.execute_input":"2024-04-12T20:27:39.813801Z","iopub.status.idle":"2024-04-12T20:27:40.129075Z","shell.execute_reply.started":"2024-04-12T20:27:39.813765Z","shell.execute_reply":"2024-04-12T20:27:40.127986Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"The gradual decrease in both loss functions over time aligns with the expected behavior of a functioning model. To visualize the actual results, we can plot random photos alongside their corresponding Monet-esque counterparts generated by our model.","metadata":{}},{"cell_type":"code","source":"_, ax = plt.subplots(2, 4, figsize=(12, 8))\nfor i in range(4):\n    photo_img, _ = next(iter(image_dl))\n    pred_monet = gan.gen_ptm(photo_img.to(device)).cpu().detach()\n    \n    photo_img = reverse_normalize(photo_img)\n    pred_monet = reverse_normalize(pred_monet)\n    \n    ax[0, i].imshow(photo_img[0].permute(1, 2, 0))\n    ax[1, i].imshow(pred_monet[0].permute(1, 2, 0))\n    ax[0, i].set_title(\"Photo\")\n    ax[1, i].set_title(\"Monet\")\n    ax[0, i].axis(\"off\")\n    ax[1, i].axis(\"off\")\n\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:27:46.270733Z","iopub.execute_input":"2024-04-12T20:27:46.271144Z","iopub.status.idle":"2024-04-12T20:27:47.268689Z","shell.execute_reply.started":"2024-04-12T20:27:46.271111Z","shell.execute_reply":"2024-04-12T20:27:47.267067Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Results\n\nNow that our model is trained, we can utilize it to generate the desired Monet-like images from the provided photos, enabling us to prepare a submission for the competition.","metadata":{}},{"cell_type":"code","source":"class PhotoDataset(Dataset):\n    def __init__(self, photo_dir, size=(256, 256), normalize=True):\n        super().__init__()\n        self.photo_dir = photo_dir\n        self.photo_idx = dict()\n        \n        if normalize:\n            self.transform = transforms.Compose([\n                transforms.Resize(size),\n                transforms.ToTensor(),\n                transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))                                \n            ])\n        else:\n            self.transform = transforms.Compose([\n                transforms.Resize(size),\n                transforms.ToTensor()                               \n            ])\n            \n        self.photo_idx = {i: fl for i, fl in enumerate(os.listdir(self.photo_dir))}\n\n    def __getitem__(self, idx):\n        photo_path = os.path.join(self.photo_dir, self.photo_idx[idx])\n        \n        photo_img = Image.open(photo_path)\n        photo_img = self.transform(photo_img)\n        \n        return photo_img\n\n    def __len__(self):\n        return len(self.photo_idx.keys())","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:27:50.544290Z","iopub.execute_input":"2024-04-12T20:27:50.545295Z","iopub.status.idle":"2024-04-12T20:27:50.555533Z","shell.execute_reply.started":"2024-04-12T20:27:50.545249Z","shell.execute_reply":"2024-04-12T20:27:50.554430Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"photo_ds = PhotoDataset(PHOTO_PATH)\nphoto_dl = DataLoader(photo_ds, batch_size=1, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:28:37.104934Z","iopub.execute_input":"2024-04-12T20:28:37.105385Z","iopub.status.idle":"2024-04-12T20:28:37.115190Z","shell.execute_reply.started":"2024-04-12T20:28:37.105351Z","shell.execute_reply":"2024-04-12T20:28:37.114313Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# If OUTPUT_PATH doesn't exist, create it\nif not os.path.exists(OUTPUT_PATH):\n    os.makedirs(OUTPUT_PATH)\n\ntrans = transforms.ToPILImage()\n\ni = 1\nfor photo in photo_dl:\n    with torch.no_grad():\n        pred_monet = gan.gen_ptm(photo.to(device)).cpu().detach()\n    \n    pred_monet = reverse_normalize(pred_monet)\n    img = trans(pred_monet[0]).convert(\"RGB\")\n    \n    img.save(os.path.join(OUTPUT_PATH, f'{i}.jpg'))\n    i += 1","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:29:37.625504Z","iopub.execute_input":"2024-04-12T20:29:37.626243Z","iopub.status.idle":"2024-04-12T20:32:44.113356Z","shell.execute_reply.started":"2024-04-12T20:29:37.626195Z","shell.execute_reply":"2024-04-12T20:32:44.112134Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"display_images_grid(OUTPUT_PATH)","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:33:02.786422Z","iopub.execute_input":"2024-04-12T20:33:02.786847Z","iopub.status.idle":"2024-04-12T20:33:04.483713Z","shell.execute_reply.started":"2024-04-12T20:33:02.786804Z","shell.execute_reply":"2024-04-12T20:33:04.482007Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"shutil.make_archive(\"/kaggle/working/images\", 'zip', \"/kaggle/images\")","metadata":{"execution":{"iopub.status.busy":"2024-04-12T20:33:27.282227Z","iopub.execute_input":"2024-04-12T20:33:27.283188Z","iopub.status.idle":"2024-04-12T20:33:27.290530Z","shell.execute_reply.started":"2024-04-12T20:33:27.283157Z","shell.execute_reply":"2024-04-12T20:33:27.289356Z"},"trusted":true},"execution_count":null,"outputs":[]}]}